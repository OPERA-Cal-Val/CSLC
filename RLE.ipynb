{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLE (relative geolocation error) estimation at randomly selected points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for calculating <B>RLE (relative geolocation error)</B> of Sentinel-1 coregistered SLCs at randomly selected points using ISCE2's Ampcor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><I>Inputs</I></b>: &emsp;   Sentinel-1 coregistered SLCs produced by COMPASS     <br>\n",
    "<b><I>Relative Orbit</I></b>: &emsp; 64 (ascending track)    <br>\n",
    "<b><I>Period</I></b>: &emsp; 2014/12/21 - 2022/11/09 (287 SLCs)<br>\n",
    "<b><I>Subswath</I></b>: &emsp; IW2 (1 burst)<br>\n",
    "<b><I>Location</I></b>: &emsp; Rosamond, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import folium\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "import h5py\n",
    "\n",
    "import isce\n",
    "from isce.components.mroipac.ampcor.Ampcor import Ampcor\n",
    "\n",
    "import isceobj\n",
    "from isceobj.Util.mathModule import is_power2\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyproj import CRS, Proj\n",
    "\n",
    "import requests\n",
    "\n",
    "import scipy\n",
    "\n",
    "import shapely.wkt as wkt\n",
    "from shapely import geometry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Read First OPERA Coregistered SLC in stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for papermill\n",
    "wdir = '/u/trappist-r0/bato/work/ROSAMOND/COMPASS_TEST/A064/stack'\n",
    "\n",
    "burst_id = 't064_135523_iw2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define polarization/path\n",
    "pol = 'VV'\n",
    "burst_path = f'{wdir}/{burst_id}'\n",
    "\n",
    "#defining dates of produced SLCs\n",
    "days = glob.glob(burst_path + '/*')\n",
    "days = [path.split('/')[-1] for path in days]\n",
    "days = sorted(days)\n",
    "\n",
    "# get first date\n",
    "date = str(days[0])\n",
    "path_h5 = f'{wdir}/{burst_id}/{date}/{burst_id}_{date}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSLC and necessary metadata\n",
    "DATA_ROOT = 'science/SENTINEL1'\n",
    "grid_path = f'{DATA_ROOT}/CSLC/grids'\n",
    "metadata_path = f'{DATA_ROOT}/CSLC/metadata'\n",
    "burstmetadata_path = f'{DATA_ROOT}/CSLC/metadata/processing_information/s1_burst_metadata'\n",
    "id_path = f'{DATA_ROOT}/identification'\n",
    "\n",
    "with h5py.File(path_h5,'r') as h5:\n",
    "    cslc = h5[f'{grid_path}/{pol}'][:]\n",
    "    xcoor = h5[f'{grid_path}/x_coordinates'][:]\n",
    "    ycoor = h5[f'{grid_path}/y_coordinates'][:]\n",
    "    dx = h5[f'{grid_path}/x_spacing'][()].astype(int)\n",
    "    dy = h5[f'{grid_path}/y_spacing'][()].astype(int)\n",
    "    rangePixelSize = abs(h5[f'{grid_path}/x_spacing'][()].astype(int))\n",
    "    azimuthPixelSize = abs(h5[f'{grid_path}/y_spacing'][()].astype(int))\n",
    "    #rangePixelSize = h5[f'{burstmetadata_path}/range_pixel_spacing'][()].astype(float)\n",
    "    #azimuthPixelSize = 13.94232 #hardcoded\n",
    "    epsg = h5[f'{grid_path}/projection'][()].astype(int)\n",
    "    sensing_start = h5[f'{burstmetadata_path}/sensing_start'][()].astype(str)\n",
    "    dims = h5[f'{burstmetadata_path}/shape'][:]\n",
    "    bounding_polygon =h5[f'{id_path}/bounding_polygon'][()].astype(str) \n",
    "    orbit_direction = h5[f'{id_path}/orbit_pass_direction'][()].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize burst location on a map\n",
    "cslc_poly = wkt.loads(bounding_polygon)\n",
    "bbox = [cslc_poly.bounds[0], cslc_poly.bounds[2], cslc_poly.bounds[1], cslc_poly.bounds[3]]\n",
    "m = folium.Map(location=[np.mean(bbox[2:3]), np.mean(bbox[0:1])], zoom_start=9, tiles='CartoDB positron')\n",
    "gdf_cslc = gpd.GeoDataFrame(index=[0], crs=f'epsg:{epsg}', geometry=[cslc_poly])\n",
    "geoj_cslc = gdf_cslc.to_json()\n",
    "geoj_cslc = folium.GeoJson(data=geoj_cslc,\n",
    "                        style_function=lambda x: {'fillColor': 'orange'})\n",
    "geoj_cslc.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CSLC\n",
    "%matplotlib widget\n",
    "#bbox = [xcoor.min(),xcoor.max(),ycoor.min(),ycoor.max()]\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.imshow(20*np.log10(np.abs(cslc)), cmap='gray',interpolation=None, extent=bbox)\n",
    "fig.suptitle(f'{burst_id}_{date}_{pol}')\n",
    "ax.set_aspect(1)\n",
    "ax.set_xlabel('Longitude (deg)')\n",
    "ax.set_ylabel('Latitude (deg)')\n",
    "fig.savefig('cslc.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating pixel location of random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pixel location of random points \n",
    "xloc = []      \n",
    "yloc = []      \n",
    "\n",
    "#interest area\n",
    "min_lat = 34.797\n",
    "max_lat = 34.806\n",
    "min_lon = -118.086\n",
    "max_lon = -118.032\n",
    "\n",
    "lat_ = [min_lat, max_lat]\n",
    "lon_ = [min_lon, max_lon]\n",
    "\n",
    "comb_ = list(itertools.product(lat_, lon_))\n",
    "\n",
    "#calculating the locations in SAR image\n",
    "UTMx = []\n",
    "UTMy = []\n",
    "xindex = []\n",
    "yindex = []\n",
    "xloc = []\n",
    "yloc = []\n",
    "_in = []\n",
    "for lat, lon in comb_:\n",
    "    _Proj = Proj(CRS.from_epsg(epsg))\n",
    "    _x, _y = _Proj(lon, lat, inverse=False)     #conversion of lat/lon of CRs to UTM coordinates\n",
    "    \n",
    "    #Indices of the CRs in SLC image\n",
    "    _xloc = int((_x-xcoor[0])/dx)    \n",
    "    _yloc = int((_y-ycoor[0])/dy)\n",
    "    \n",
    "    UTMx.append(_x) \n",
    "    UTMy.append(_y)\n",
    "    xindex.append(_xloc)\n",
    "    yindex.append(_yloc)\n",
    "    xloc.append((_x-xcoor[0])/dx)\n",
    "    yloc.append((_y-ycoor[0])/dy)\n",
    "    _in.append(cslc_poly.contains(geometry.Point(lon, lat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting random locations and calculating their pixel location in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_x = min(xloc); max_x = max(xloc); min_y = min(yloc); max_y = max(yloc)\n",
    "\n",
    "n_pixels = 20   #number of random pixels\n",
    "randX = np.random.randint((min_x),(max_x), n_pixels)  \n",
    "randY = np.random.randint((min_y),(max_y), n_pixels)\n",
    "\n",
    "df = pd.DataFrame(data={'ID':np.arange(n_pixels),'xloc':randX,'yloc':randY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offset calculation using ampcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmpcorPrep:\n",
    "    #estimating offsets from two chips using ampcor\n",
    "    \n",
    "    def __init__(self, df, days, derampPath, n_neighbor, winWidth, winHeight, \n",
    "                 searchWidth, searchHeight, ovs, xstep, ystep):\n",
    "\n",
    "        self.df = df   #pandas table\n",
    "        self.days = days\n",
    "        self.derampPath = derampPath \n",
    "\n",
    "        self.n_neighbor = n_neighbor\n",
    "        self.winWidth = winWidth\n",
    "        self.winHeight = winHeight\n",
    "        self.xhalfwin = int(winWidth/2) + searchWidth\n",
    "        self.yhalfwin = int(winHeight/2) + searchHeight\n",
    "\n",
    "        self.searchWidth = searchWidth\n",
    "        self.searchHeight = searchHeight\n",
    "        self.ovs = ovs   #oversampling factor\n",
    "        self.xstep = xstep\n",
    "        self.ystep = ystep\n",
    "\n",
    "        self.__check_inputs__()\n",
    "        \n",
    "    def __check_inputs__(self): \n",
    "        if not is_power2(self.winWidth):\n",
    "            raise ValueError('Window size needs to be power of 2.') \n",
    "\n",
    "        if not is_power2(self.winHeight):\n",
    "            raise ValueError('Window size needs to be power of 2.')\n",
    "         \n",
    "        if not is_power2(self.ovs):\n",
    "            raise ValueError('Oversampling factor needs to be power of 2.')\n",
    "\n",
    "        if (self.searchWidth%2 == 1):\n",
    "            raise ValueError('Search window size width needs to be multiple of 2.')\n",
    "\n",
    "        if (self.searchHeight%2 == 1):\n",
    "            raise ValueError('Search window size width needs to be multiple of 2.')\n",
    "\n",
    "    def run(self):\n",
    "        ''' \n",
    "          finding correlation peak with ampcor\n",
    "        '''\n",
    "       \n",
    "        def __create_cpxFile__(filename,dat,width,length):\n",
    "            _img = isceobj.Image.createImage()\n",
    "            _img.setFilename(filename)\n",
    "            _img.setWidth(width)\n",
    "            _img.setLength(length)\n",
    "            _img.setAccessMode('write')\n",
    "            _img.bands = 1\n",
    "            DataType = 'CFLOAT'\n",
    "            outtype = '<f'  #little endian (float)\n",
    "            _img.dataType = DataType\n",
    "            _img.scheme = 'BIP'\n",
    "            _img.renderHdr()\n",
    "            _img.renderVRT()\n",
    "            _img.finalizeImage()\n",
    "\n",
    "            fout = open(filename, \"wb\")\n",
    "            dat_cpx = np.zeros((length,2*width))\n",
    "            dat_cpx[:,::2] = np.real(dat)\n",
    "            dat_cpx[:,1::2] = np.imag(dat)\n",
    "            dat_cpx.astype(outtype).tofile(fout)   #little endian\n",
    "            _img = None; dat_cpx = None\n",
    " \n",
    "        if not os.path.isdir('./cross_correlation_random_points'):\n",
    "            os.mkdir('./cross_correlation_random_points')\n",
    "\n",
    "        for i,ref_day in enumerate(self.days):\n",
    "            ref_path = f'{wdir}/{burst_id}/{ref_day}/{burst_id}_{date}.h5'\n",
    "            sec_days = self.days[i+1:min(len(self.days),i+1+self.n_neighbor)]\n",
    "\n",
    "            for sec_day in sec_days:\n",
    "                sec_path = f'{wdir}/{burst_id}/{sec_day}/{burst_id}_{date}.h5'\n",
    "                print(f'Ref_image: {ref_day}; Sec_image: {sec_day}')\n",
    "                if os.path.isfile(f'./cross_correlation_random_points/{ref_day}_{sec_day}.csv'):\n",
    "                    print(f'./cross_correlation_random_points/{ref_day}_{sec_day}.csv exist!')\n",
    "                    continue\n",
    "                \n",
    "                dx = []\n",
    "                dy = []\n",
    "                snr = [] \n",
    "                for xoff, yoff in zip(self.df['xloc'],self.df['yloc']):\n",
    "                    ref_slc = h5py.File(path_h5,'r')[f'{grid_path}']['VV']\n",
    "                    sec_slc = h5py.File(path_h5,'r')[f'{grid_path}']['VV']\n",
    "\n",
    "                    slccrop_ref = ref_slc[(yoff-self.yhalfwin+1):(yoff+self.yhalfwin),\n",
    "                                                 (xoff-self.xhalfwin+1):(xoff+self.xhalfwin)]\n",
    "                    slccrop_sec = sec_slc[(yoff-self.yhalfwin+1):(yoff+self.yhalfwin),\n",
    "                                                 (xoff-self.xhalfwin+1):(xoff+self.xhalfwin)] \n",
    "                    (chiplength, chipwidth) = slccrop_ref.shape\n",
    "\n",
    "                    objAmpcor = Ampcor()\n",
    "                    objAmpcor.configure()\n",
    "                    objAmpcor.setImageDataType1('complex')\n",
    "                    objAmpcor.setImageDataType2('complex')\n",
    "                    objAmpcor.acrossGrossOffset = 0\n",
    "                    objAmpcor.downGrossOffset = 0\n",
    "\n",
    "                    objAmpcor.windowSizeWidth = self.winWidth\n",
    "                    objAmpcor.windowSizeHeight = self.winHeight\n",
    "                    objAmpcor.searchWindowSizeWidth = self.searchWidth\n",
    "                    objAmpcor.searchWindowSizeHeight = self.searchHeight\n",
    "                    objAmpcor.oversamplingFactor = self.ovs   \n",
    "                    \n",
    "                    objAmpcor.setFirstPRF(1.0) \n",
    "                    objAmpcor.setSecondPRF(1.0)\n",
    "                    objAmpcor.setFirstRangeSpacing(1.0)\n",
    "                    objAmpcor.setSecondRangeSpacing(1.0)\n",
    "\n",
    "                    #saving reference and secondary chips\n",
    "                    refFile = 'ref.dat'; refXML = refFile + '.xml'; refVRT = refFile + '.vrt'\n",
    "                    __create_cpxFile__(refFile,slccrop_ref,chipwidth,chiplength)\n",
    "\n",
    "                    secFile = 'sec.dat'; secXML = secFile + '.xml'; secVRT = secFile + '.vrt'\n",
    "                    __create_cpxFile__(secFile,slccrop_sec,chipwidth,chiplength)\n",
    "\n",
    "                    #loading saved chips \n",
    "                    referenceImg = isceobj.createImage()   #Empty image\n",
    "                    referenceImg.load(refXML) #Load from XML file\n",
    "                    referenceImg.setAccessMode('read')     #Set it up for reading \n",
    "                    referenceImg.createImage()             #Create File\n",
    "\n",
    "                    secondaryImg = isceobj.createImage()    #Empty image\n",
    "                    secondaryImg.load(secXML)   #Load it from XML file\n",
    "                    secondaryImg.setAccessMode('read')      #Set it up for reading\n",
    "                    secondaryImg.createImage()              #Create File\n",
    "\n",
    "                    #ampcor: amplitude cross-correlation\n",
    "                    objAmpcor.ampcor(referenceImg,secondaryImg)\n",
    "                    clear_output(wait=False)   #clearing cell outputs\n",
    "\n",
    "                    referenceImg.finalizeImage()\n",
    "                    secondaryImg.finalizeImage()\n",
    "                    referenceImg = None\n",
    "                    secondaryImg = None\n",
    "\n",
    "                    os.remove(refFile); os.remove(refXML); os.remove(refVRT)\n",
    "                    os.remove(secFile); os.remove(secXML); os.remove(secVRT)\n",
    "\n",
    "                    offField = objAmpcor.getOffsetField()  #output of ampcor \n",
    "                    field = np.array(offField.unpackOffsets())\n",
    "\n",
    "                    if (field.size == 0): # if ampcor fail, set the snr to 0\n",
    "                        dx.append(0.0);dy.append(0.0);snr.append(0.0)\n",
    "                    else:\n",
    "                                   \n",
    "                        success = 0\n",
    "                        for item in field:\n",
    "                            if (item[0]==self.xhalfwin+1) and (item[2]==self.yhalfwin+1):\n",
    "                                # when the field contains the offset of the patch we needs.\n",
    "                                dx.append(item[1]),dy.append(item[3]),snr.append(item[4])\n",
    "                                success = 1; break\n",
    "                        if success!=1:\n",
    "                            # the field do not contains the offset of the patch we needs. We average the offset field.\n",
    "                            dx.append(field[:,1].mean());dy.append(field[:,3].mean());\n",
    "                            snr.append(field[:,4].mean())\n",
    "\n",
    "                offset_df = pd.DataFrame({'ID':list(self.df['ID']),'dx':dx,'dy':dy,'SNR':snr})\n",
    "                offset_df['dx'] = offset_df['dx']*self.xstep\n",
    "                offset_df['dy'] = offset_df['dy']*self.ystep\n",
    "\n",
    "                offset_df.to_csv(f'./cross_correlation_random_points/{ref_day}_{sec_day}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running ampcor to find offsets\n",
    "objOffset = AmpcorPrep(df, days, derampPath=burst_path,n_neighbor=3,winWidth=128,\n",
    "                                winHeight=128,searchWidth=4,searchHeight=4,ovs=64,\n",
    "                                xstep=rangePixelSize,ystep=azimuthPixelSize)\n",
    "'''\n",
    "     n_neighbor: number of neighbor pairs for calculating offsets\n",
    "     winWidth, winHeight: window size for offset estimation\n",
    "     searchWidth, searchHeight: search window size for offset estimation\n",
    "     ovs: oversampling factor for correlation\n",
    "     xstep, ystep: pixel spacing (m) in x/y\n",
    "'''\n",
    "objOffset.run()     #running ampcor\n",
    "\n",
    "clear_output(wait=False)   #clearing cell outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading offsets from csv files\n",
    "az_off = dict(); rng_off = dict(); snr = dict()\n",
    "\n",
    "for ID in df['ID']:\n",
    "    _az_off = np.empty((len(days),len(days)))\n",
    "    _az_off[:] = np.nan\n",
    "    _rng_off = _az_off.copy()\n",
    "    _rng_off[:] = np.nan\n",
    "    \n",
    "    _snr = np.zeros_like(_az_off)\n",
    "    az_off[ID] = _az_off\n",
    "    rng_off[ID] = _rng_off\n",
    "    snr[ID] = _snr\n",
    "    \n",
    "for csv_file_path in glob.glob('./cross_correlation_random_points/*.csv'):\n",
    "    csv_file_name = csv_file_path.split('/')[-1]\n",
    "    ref_day = csv_file_name[0:8]   #reference date\n",
    "    sec_day = csv_file_name[9:17]  #secondary date\n",
    "    ref_i = days.index(ref_day)    #index of reference date \n",
    "    sec_i = days.index(sec_day)    #index of secondary date\n",
    "    \n",
    "    _df = pd.read_csv(csv_file_path)\n",
    "    for ID,dxi,dyi,_snr in zip(_df['ID'],_df['dx'],_df['dy'],_df['SNR']):\n",
    "        az_off[ID][ref_i,sec_i] = dyi\n",
    "        rng_off[ID][ref_i,sec_i] = dxi\n",
    "        snr[ID][ref_i,sec_i] = _snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all calculated range and azimuth offsets \n",
    "all_az_off = []; all_rng_off = []; all_snr = []\n",
    "\n",
    "for ii in az_off.keys():\n",
    "    all_az_off.append(np.concatenate(az_off[ii]))\n",
    "    all_rng_off.append(np.concatenate(rng_off[ii]))\n",
    "    all_snr.append(np.concatenate(snr[ii]))\n",
    "\n",
    "all_az_off = np.concatenate(all_az_off)\n",
    "all_rng_off = np.concatenate(all_rng_off)\n",
    "all_snr = np.concatenate(all_snr)\n",
    "\n",
    "snr_threshold = 1.   #snr threshold\n",
    "\n",
    "df_RLE = pd.DataFrame({'rng_off':all_rng_off, 'az_off':all_az_off, 'snr':all_snr})\n",
    "df_RLE = df_RLE[(df_RLE['rng_off']!=np.nan) & (df_RLE['az_off']!=np.nan) \n",
    "                & (df_RLE['snr']>snr_threshold)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying RLEs at random points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_rqmt = 0.5  #OPERA requirements for range offsets (0.25 m)\n",
    "az_rqmt = 1.5   #OPERA requirements for azimuth offsets (0.75 m)  \n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(15,15))\n",
    "sc = ax.scatter(df_RLE['rng_off'],df_RLE['az_off'],s=20)\n",
    "rect = patches.Rectangle((-rng_rqmt/2,-az_rqmt/2),rng_rqmt,az_rqmt,\n",
    "                         linewidth=1,edgecolor='r',facecolor='none') \n",
    "ax.add_patch(rect)   #box displaying OPERA requirements\n",
    "ax.grid(True)\n",
    "ax.set_xlim(-2,2)\n",
    "ax.set_ylim(-2,2)\n",
    "ax.axhline(0, color='black')\n",
    "ax.axvline(0, color='black')\n",
    "ax.set_title('Relative geolocation error at random points', fontsize=24)\n",
    "ax.set_xlabel('$\\Delta$ Range (m)', fontsize=18)\n",
    "ax.set_ylabel('$\\Delta$ Azimuth (m)', fontsize=18)\n",
    "rngMean = np.mean(df_RLE['rng_off'])\n",
    "rngStd = np.std(df_RLE['rng_off'])\n",
    "azMean = np.mean(df_RLE['az_off'])\n",
    "azStd = np.std(df_RLE['az_off'])\n",
    "ax.errorbar(rngMean,azMean,yerr=azStd,xerr=rngStd,color='w',lw=2,alpha=0.8,capsize=6)\n",
    "sc_mean = ax.scatter(rngMean,azMean,alpha=0.8,c='r',s=160,marker='*')\n",
    "ax.legend([sc,sc_mean,rect],['RLEs','Mean of RLEs','OPERA requirements'], fontsize=18,\n",
    "          frameon=False,handletextpad=0.3)\n",
    "fig.savefig('RLE.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean RLE in range: ',np.mean(df_RLE['rng_off']), 'std RLE in range: ',np.std(df_RLE['rng_off']))\n",
    "print('mean RLE in azimuth: ',np.mean(df_RLE['az_off']), 'std RLE in azimuth: ',np.std(df_RLE['az_off']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calval_CSLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "d8fe6e63a564e74543fd20f912c92a13618d07e34fff403be16d439e5c371ed0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
